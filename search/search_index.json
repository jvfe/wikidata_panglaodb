{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Analysing the extent of cell type information present in Wikidata: A case study on PanglaoDB Research compendium for the project \"Analysing the extent of cell type information present in Wikidata: A case study on PanglaoDB\". Repository brief descrition Research-related directories: analysis : Scripts and notebooks used for the main analysis. It also includes the subdirectories: data : Raw data from PanglaoDB and Wikidata is stored here. results : Processed data is stored here. manuscripts : Manuscripts for this research project, each manuscript is a submodule of a GitHub repository that uses Manubot . Software-related directories, they are structured similarly to a Python package: wikidata_panglaodb : This is the source code for all author-defined functions used in the analysis. tests : These are the unit tests for the wikidata_panglaodb \"package\" functions. docs : This is a directory containing documentation for the wikidata_panglaodb functions, it is served as a live website in our github pages branch. Reproducing and developing Reproducing the analyses Pre-requisites: Python>=3.7 A unix based terminal interface. Download the repository's zip file or clone it using: git clone --recurse-submodules https://github.com/jvfe/wikidata_panglaodb Then, at the project's root directory (wikidata_panglaodb/): pip install . And then run the scripts in the analysis/ subdirectory. Collaborating Pre-requisites: Git A unix based terminal interface Conda Initiate the environment: conda env create -f environment.yaml conda activate wdt_panglaodb Download the base package in editable mode: pip install -e . If you've already collaborated before but changes have been made to the conda enviroment/repository, run: git pull origin master conda activate wdt_panglaodb conda env update --file environment.yaml","title":"Home"},{"location":"#analysing-the-extent-of-cell-type-information-present-in-wikidata-a-case-study-on-panglaodb","text":"Research compendium for the project \"Analysing the extent of cell type information present in Wikidata: A case study on PanglaoDB\".","title":"Analysing the extent of cell type information present in Wikidata: A case study on PanglaoDB"},{"location":"#repository-brief-descrition","text":"Research-related directories: analysis : Scripts and notebooks used for the main analysis. It also includes the subdirectories: data : Raw data from PanglaoDB and Wikidata is stored here. results : Processed data is stored here. manuscripts : Manuscripts for this research project, each manuscript is a submodule of a GitHub repository that uses Manubot . Software-related directories, they are structured similarly to a Python package: wikidata_panglaodb : This is the source code for all author-defined functions used in the analysis. tests : These are the unit tests for the wikidata_panglaodb \"package\" functions. docs : This is a directory containing documentation for the wikidata_panglaodb functions, it is served as a live website in our github pages branch.","title":"Repository brief descrition"},{"location":"#reproducing-and-developing","text":"","title":"Reproducing and developing"},{"location":"#reproducing-the-analyses","text":"Pre-requisites: Python>=3.7 A unix based terminal interface. Download the repository's zip file or clone it using: git clone --recurse-submodules https://github.com/jvfe/wikidata_panglaodb Then, at the project's root directory (wikidata_panglaodb/): pip install . And then run the scripts in the analysis/ subdirectory.","title":"Reproducing the analyses"},{"location":"#collaborating","text":"Pre-requisites: Git A unix based terminal interface Conda Initiate the environment: conda env create -f environment.yaml conda activate wdt_panglaodb Download the base package in editable mode: pip install -e . If you've already collaborated before but changes have been made to the conda enviroment/repository, run: git pull origin master conda activate wdt_panglaodb conda env update --file environment.yaml","title":"Collaborating"},{"location":"analysis_description/","text":"Analysis directory Here you will find all scripts used in the actual analysis. To run them, make sure you ran pip install . at the root directory. Simple data transformation scripts are plain text python files, scripts containing extensive information and associated plots are jupyter notebooks. Brief description preprocessing.py : Downloads metadata from PanglaoDB into the data/panglaodb directory, also reconciles said data and outputs the matches to the results/all_matches directory. For genes, since a manual merge is used, the matches are outputted to results/true_matches. similarity_check.py : Checks the reconciled/all_matches data for similarity matching, outputting the filtered data to results/true_matches. item_quality.py : Item quality assesment on matches and associated plots.","title":"Analysis description"},{"location":"analysis_description/#analysis-directory","text":"Here you will find all scripts used in the actual analysis. To run them, make sure you ran pip install . at the root directory. Simple data transformation scripts are plain text python files, scripts containing extensive information and associated plots are jupyter notebooks.","title":"Analysis directory"},{"location":"analysis_description/#brief-description","text":"preprocessing.py : Downloads metadata from PanglaoDB into the data/panglaodb directory, also reconciles said data and outputs the matches to the results/all_matches directory. For genes, since a manual merge is used, the matches are outputted to results/true_matches. similarity_check.py : Checks the reconciled/all_matches data for similarity matching, outputting the filtered data to results/true_matches. item_quality.py : Item quality assesment on matches and associated plots.","title":"Brief description"},{"location":"log_book/","text":"What we did, tried, and accomplished during each day of the project. 03/09 Downloaded cell type, organ, tissue and gene data from the wikidata query service. Gene queries used: Homo sapiens , Mus musculus Cell query used Tissues query used Organs query used 04/09 Reconciled cell type, tissue and organ using the preprocessing script and associated functions, returning all matches. 05/09 Made a script, similarity_check.py, to better match Panglao's items to wikidata QIDs, ran the script and the results are in results/true_matches/. 06/09 Refactored the similarity function, to use less repetition Added matching section for genes in preprocessing.py, the matching is done using a manual merge with the wikidata query. 07/09 Started work on manual item checking Started working on the item quality assesment - halted until manual item matching is finished","title":"Log book"},{"location":"log_book/#0309","text":"Downloaded cell type, organ, tissue and gene data from the wikidata query service. Gene queries used: Homo sapiens , Mus musculus Cell query used Tissues query used Organs query used","title":"03/09"},{"location":"log_book/#0409","text":"Reconciled cell type, tissue and organ using the preprocessing script and associated functions, returning all matches.","title":"04/09"},{"location":"log_book/#0509","text":"Made a script, similarity_check.py, to better match Panglao's items to wikidata QIDs, ran the script and the results are in results/true_matches/.","title":"05/09"},{"location":"log_book/#0609","text":"Refactored the similarity function, to use less repetition Added matching section for genes in preprocessing.py, the matching is done using a manual merge with the wikidata query.","title":"06/09"},{"location":"log_book/#0709","text":"Started work on manual item checking Started working on the item quality assesment - halted until manual item matching is finished","title":"07/09"},{"location":"reference/preprocessing/","text":"Preprocessing module downloads_panglao() Downloads metadata from PanglaoDB Gets the unique values of metadata entities and writes those to text files. Parameters: Name Type Description Default data_urls dict(list A dictionary with the urls of each metadata file. required Returns: Type Description tuple A tuple containing the DataFrames tissues, genes, cells_organs_germlayers and cells_w_descriptions. Source code in wikidata_panglaodb/pre.py def downloads_panglao ( data_urls ): \"\"\"Downloads metadata from PanglaoDB Gets the unique values of metadata entities and writes those to text files. Args: data_urls (dict(list)): A dictionary with the urls of each metadata file. Returns: tuple: A tuple containing the DataFrames tissues, genes, cells_organs_germlayers and cells_w_descriptions. \"\"\" tissues = pd . read_csv ( data_urls [ \"tissues\" ], usecols = [ 2 , 4 ], names = [ \"tissue\" , \"species\" ] ) . drop_duplicates () genes = pd . read_csv ( data_urls [ \"genes\" ], names = [ \"ensg_panglao\" , \"symbol\" ]) genes [ \"species\" ] = np . where ( genes [ \"ensg_panglao\" ] . str . startswith ( \"ENSMUS\" ), \"Mus musculus\" , \"Homo sapiens\" ) cells_organs_germlayers = pd . read_csv ( data_urls [ \"cells_organs_germlayers\" ], names = [ \"cell_type\" , \"germ_layer\" , \"organ\" ], ) cells_w_descriptions = pd . read_csv ( data_urls [ \"cells_w_descriptions\" ], names = [ \"cell_type\" , \"description\" , \"synonyms\" ], ) return tissues , genes , cells_organs_germlayers , cells_w_descriptions reconcile_more_types() Reconcile dataframe column against one type QID or more This functions loops through all qids given in the type_qids list and reconciles the pandas column to them, returning a concatenated dataframe with all the matches. Parameters: Name Type Description Default dataframe_column Series A pandas dataframe column with the values to reconcile. required type_qids list A list of the QIDs value you want to reconcile against. required Returns: DataFrame: A dataframe containing all possible matches for each item type. Source code in wikidata_panglaodb/pre.py def reconcile_more_types ( dataframe_column , type_qids ): \"\"\"Reconcile dataframe column against one type QID or more This functions loops through all qids given in the type_qids list and reconciles the pandas column to them, returning a concatenated dataframe with all the matches. Args: dataframe_column (Series): A pandas dataframe column with the values to reconcile. type_qids (list): A list of the QIDs value you want to reconcile against. Returns: DataFrame: A dataframe containing all possible matches for each item type. \"\"\" all_matches = [] for type in type_qids : try : current = reconcile ( dataframe_column , type_id = type , top_res = \"all\" ) all_matches . append ( current ) except Exception : pass try : full_df_matches = pd . concat ( all_matches ) except ValueError : pass else : return full_df_matches","title":"Preprocessing"},{"location":"reference/preprocessing/#preprocessing-module","text":"downloads_panglao()","title":"Preprocessing module"},{"location":"reference/preprocessing/#wikidata_panglaodb.pre.downloads_panglao","text":"Downloads metadata from PanglaoDB Gets the unique values of metadata entities and writes those to text files. Parameters: Name Type Description Default data_urls dict(list A dictionary with the urls of each metadata file. required Returns: Type Description tuple A tuple containing the DataFrames tissues, genes, cells_organs_germlayers and cells_w_descriptions. Source code in wikidata_panglaodb/pre.py def downloads_panglao ( data_urls ): \"\"\"Downloads metadata from PanglaoDB Gets the unique values of metadata entities and writes those to text files. Args: data_urls (dict(list)): A dictionary with the urls of each metadata file. Returns: tuple: A tuple containing the DataFrames tissues, genes, cells_organs_germlayers and cells_w_descriptions. \"\"\" tissues = pd . read_csv ( data_urls [ \"tissues\" ], usecols = [ 2 , 4 ], names = [ \"tissue\" , \"species\" ] ) . drop_duplicates () genes = pd . read_csv ( data_urls [ \"genes\" ], names = [ \"ensg_panglao\" , \"symbol\" ]) genes [ \"species\" ] = np . where ( genes [ \"ensg_panglao\" ] . str . startswith ( \"ENSMUS\" ), \"Mus musculus\" , \"Homo sapiens\" ) cells_organs_germlayers = pd . read_csv ( data_urls [ \"cells_organs_germlayers\" ], names = [ \"cell_type\" , \"germ_layer\" , \"organ\" ], ) cells_w_descriptions = pd . read_csv ( data_urls [ \"cells_w_descriptions\" ], names = [ \"cell_type\" , \"description\" , \"synonyms\" ], ) return tissues , genes , cells_organs_germlayers , cells_w_descriptions reconcile_more_types()","title":"wikidata_panglaodb.pre.downloads_panglao"},{"location":"reference/preprocessing/#wikidata_panglaodb.pre.reconcile_more_types","text":"Reconcile dataframe column against one type QID or more This functions loops through all qids given in the type_qids list and reconciles the pandas column to them, returning a concatenated dataframe with all the matches. Parameters: Name Type Description Default dataframe_column Series A pandas dataframe column with the values to reconcile. required type_qids list A list of the QIDs value you want to reconcile against. required Returns: DataFrame: A dataframe containing all possible matches for each item type. Source code in wikidata_panglaodb/pre.py def reconcile_more_types ( dataframe_column , type_qids ): \"\"\"Reconcile dataframe column against one type QID or more This functions loops through all qids given in the type_qids list and reconciles the pandas column to them, returning a concatenated dataframe with all the matches. Args: dataframe_column (Series): A pandas dataframe column with the values to reconcile. type_qids (list): A list of the QIDs value you want to reconcile against. Returns: DataFrame: A dataframe containing all possible matches for each item type. \"\"\" all_matches = [] for type in type_qids : try : current = reconcile ( dataframe_column , type_id = type , top_res = \"all\" ) all_matches . append ( current ) except Exception : pass try : full_df_matches = pd . concat ( all_matches ) except ValueError : pass else : return full_df_matches","title":"wikidata_panglaodb.pre.reconcile_more_types"},{"location":"reference/similarity/","text":"Similarity module get_string_match() Checks if the stemmed version of two strings is the same Sometimes matches from the reconciliation service return as false since the item has few statements or no statements at all. To take care of those cases we'll perform a simple string similarity check, using the stemmed version of both strings. Parameters: Name Type Description Default string1 str A string to compare. required string2 str A string to compare. required Returns: Type Description bool If they match, return True, else return False. Source code in wikidata_panglaodb/similarity.py def get_string_match ( string1 , string2 ): \"\"\"Checks if the stemmed version of two strings is the same Sometimes matches from the reconciliation service return as false since the item has few statements or no statements at all. To take care of those cases we'll perform a simple string similarity check, using the stemmed version of both strings. Args: string1 (str): A string to compare. string2 (str): A string to compare. Returns: bool: If they match, return True, else return False. \"\"\" tokenized = [[ tokenized ] for tokenized in [ string1 , string2 ]] ps = PorterStemmer () stemmed = [[ ps . stem ( w )] for tokens in tokenized for w in tokens ] return stemmed [ 0 ] == stemmed [ 1 ]","title":"Similarity"},{"location":"reference/similarity/#similarity-module","text":"get_string_match()","title":"Similarity module"},{"location":"reference/similarity/#wikidata_panglaodb.similarity.get_string_match","text":"Checks if the stemmed version of two strings is the same Sometimes matches from the reconciliation service return as false since the item has few statements or no statements at all. To take care of those cases we'll perform a simple string similarity check, using the stemmed version of both strings. Parameters: Name Type Description Default string1 str A string to compare. required string2 str A string to compare. required Returns: Type Description bool If they match, return True, else return False. Source code in wikidata_panglaodb/similarity.py def get_string_match ( string1 , string2 ): \"\"\"Checks if the stemmed version of two strings is the same Sometimes matches from the reconciliation service return as false since the item has few statements or no statements at all. To take care of those cases we'll perform a simple string similarity check, using the stemmed version of both strings. Args: string1 (str): A string to compare. string2 (str): A string to compare. Returns: bool: If they match, return True, else return False. \"\"\" tokenized = [[ tokenized ] for tokenized in [ string1 , string2 ]] ps = PorterStemmer () stemmed = [[ ps . stem ( w )] for tokens in tokenized for w in tokens ] return stemmed [ 0 ] == stemmed [ 1 ]","title":"wikidata_panglaodb.similarity.get_string_match"},{"location":"reference/wdt/","text":"Wikidata module get_number_of_statements_for_items() Return a pandas dataframe of items and their number of statements This function takes in a list of QIDs and uses the Wikibase API to return a table with the number of statements each item has. Parameters: Name Type Description Default qid_list list A list containing the QIDs you want to analyse. required Returns: Type Description DataFrame A dataframe of two columns, one for the input QIDs, another with the number of statements for each QID. Source code in wikidata_panglaodb/wdt.py def get_number_of_statements_for_items ( qid_list , has_property ): \"\"\"Return a pandas dataframe of items and their number of statements This function takes in a list of QIDs and uses the Wikibase API to return a table with the number of statements each item has. Args: qid_list (list): A list containing the QIDs you want to analyse. Returns: DataFrame: A dataframe of two columns, one for the input QIDs, another with the number of statements for each QID. \"\"\" # API limits to 50 items at once item_quality_dict = defaultdict ( list ) for i in range ( 0 , len ( qid_list ), 50 ): curr_ids = qid_list [ i : ( i + 50 )] params = { \"action\" : \"wbgetentities\" , \"ids\" : \"|\" . join ( curr_ids ), \"format\" : \"json\" , } resp = requests . post ( \"https://www.wikidata.org/w/api.php\" , data = params ) query_result = resp . json ()[ \"entities\" ] for item in query_result : properties = query_result [ item ][ \"claims\" ] if has_property in properties : item_quality_dict [ item ] = [ len ( properties . keys ()), True ] else : item_quality_dict [ item ] = [ len ( properties . keys ()), False ] item_quality_table = ( pd . DataFrame . from_dict ( item_quality_dict , columns = [ \"n_statements\" , \"has_property\" ], orient = \"index\" ) . rename_axis ( \"id\" ) . reset_index () ) return item_quality_table","title":"Wikidata"},{"location":"reference/wdt/#wikidata-module","text":"get_number_of_statements_for_items()","title":"Wikidata module"},{"location":"reference/wdt/#wikidata_panglaodb.wdt.get_number_of_statements_for_items","text":"Return a pandas dataframe of items and their number of statements This function takes in a list of QIDs and uses the Wikibase API to return a table with the number of statements each item has. Parameters: Name Type Description Default qid_list list A list containing the QIDs you want to analyse. required Returns: Type Description DataFrame A dataframe of two columns, one for the input QIDs, another with the number of statements for each QID. Source code in wikidata_panglaodb/wdt.py def get_number_of_statements_for_items ( qid_list , has_property ): \"\"\"Return a pandas dataframe of items and their number of statements This function takes in a list of QIDs and uses the Wikibase API to return a table with the number of statements each item has. Args: qid_list (list): A list containing the QIDs you want to analyse. Returns: DataFrame: A dataframe of two columns, one for the input QIDs, another with the number of statements for each QID. \"\"\" # API limits to 50 items at once item_quality_dict = defaultdict ( list ) for i in range ( 0 , len ( qid_list ), 50 ): curr_ids = qid_list [ i : ( i + 50 )] params = { \"action\" : \"wbgetentities\" , \"ids\" : \"|\" . join ( curr_ids ), \"format\" : \"json\" , } resp = requests . post ( \"https://www.wikidata.org/w/api.php\" , data = params ) query_result = resp . json ()[ \"entities\" ] for item in query_result : properties = query_result [ item ][ \"claims\" ] if has_property in properties : item_quality_dict [ item ] = [ len ( properties . keys ()), True ] else : item_quality_dict [ item ] = [ len ( properties . keys ()), False ] item_quality_table = ( pd . DataFrame . from_dict ( item_quality_dict , columns = [ \"n_statements\" , \"has_property\" ], orient = \"index\" ) . rename_axis ( \"id\" ) . reset_index () ) return item_quality_table","title":"wikidata_panglaodb.wdt.get_number_of_statements_for_items"}]}